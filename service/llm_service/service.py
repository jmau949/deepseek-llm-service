"""
LLM gRPC Service Implementation
------------------------------
Implementation of the LLM gRPC service.
"""

import time
import logging
import grpc
import os
import sys
from concurrent import futures
from typing import Dict, Any

from llm_service.config import Config
from llm_service.utils.ollama import OllamaClient, LLMChunk

# Import generated protocol buffer code
# This will be generated by scripts/generate_proto.py
try:
    from proto import llm_pb2, llm_pb2_grpc
except ImportError:
    raise ImportError(
        "Protocol buffer code not found. "
        "Please run 'python scripts/generate_proto.py' first."
    )

# Try to import reflection at module level to catch any import errors early
try:
    from grpc_reflection.v1alpha import reflection
    REFLECTION_AVAILABLE = True
except ImportError:
    REFLECTION_AVAILABLE = False
    logging.warning("grpc_reflection module not available. Reflection will be disabled.")
    logging.warning("Install with: pip install grpcio-reflection==<same-version-as-grpcio>")

logger = logging.getLogger(__name__)


class LLMService(llm_pb2_grpc.LLMServiceServicer):
    """
    gRPC service implementation for LLM inference.
    
    This service provides two endpoints:
    - GenerateStream: Streams the LLM response as it's generated
    - Generate: Returns the complete LLM response
    """
    
    def __init__(self, config: Config):
        """
        Initialize the LLM service with configuration.
        
        Args:
            config: Configuration object containing all settings
        """
        self.config = config
        self.ollama_client = OllamaClient(
            base_url=config.ollama_url,
            model_name=config.model_name,
            timeout=config.request_timeout
        )
        logger.info(
            f"Initialized LLM service with Ollama at {config.ollama_url} "
            f"using model {config.model_name}"
        )

    def GenerateStream(self, request, context):
        """
        Streams the LLM response back to the client.
        
        Args:
            request: The LLMRequest containing the prompt and parameters
            context: The gRPC context
            
        Yields:
            LLMResponse chunks as they are generated
        """
        logger.info(f"Received streaming request with prompt: {request.prompt[:50]}...")
        
        try:
            # Extract or create session ID for sticky sessions
            session_id = self._get_or_create_session_id(context)
            logger.debug(f"Using session ID: {session_id}")
            
            # Set session cookie in metadata for sticky sessions
            if self.config.sticky_session_enabled and session_id:
                context.set_trailing_metadata([
                    ('set-cookie', f'{self.config.sticky_session_cookie}={session_id}; Path=/; Max-Age=900; Secure; HttpOnly')
                ])
            
            # Map gRPC request parameters to Ollama parameters
            ollama_params = self._map_parameters(request.parameters)
            
            # Stream responses from Ollama
            for chunk in self.ollama_client.generate_stream(request.prompt, ollama_params):
                yield llm_pb2.LLMResponse(
                    text=chunk.text,
                    is_complete=chunk.is_complete
                )
                
            logger.info("Streaming response completed")
            
        except Exception as e:
            logger.error(f"Error generating streaming response: {e}")
            context.abort(grpc.StatusCode.INTERNAL, f"Error generating response: {e}")

    def Generate(self, request, context):
        """
        Generates a complete LLM response.
        
        Args:
            request: The LLMRequest containing the prompt and parameters
            context: The gRPC context
            
        Returns:
            LLMCompleteResponse containing the full generated text
        """
        logger.info(f"Received non-streaming request with prompt: {request.prompt[:50]}...")
        
        try:
            # Extract or create session ID for sticky sessions
            session_id = self._get_or_create_session_id(context)
            logger.debug(f"Using session ID: {session_id}")
            
            # Set session cookie in metadata for sticky sessions
            if self.config.sticky_session_enabled and session_id:
                context.set_trailing_metadata([
                    ('set-cookie', f'{self.config.sticky_session_cookie}={session_id}; Path=/; Max-Age=900; Secure; HttpOnly')
                ])
            
            # Map gRPC request parameters to Ollama parameters
            ollama_params = self._map_parameters(request.parameters)
            
            # Get complete response from Ollama
            response = self.ollama_client.generate(request.prompt, ollama_params)
            
            return llm_pb2.LLMCompleteResponse(text=response)
            
        except Exception as e:
            logger.error(f"Error generating complete response: {e}")
            context.abort(grpc.StatusCode.INTERNAL, f"Error generating response: {e}")

    def HealthCheck(self, request, context):
        """
        Simple health check method that always succeeds if the service is running.
        
        Args:
            request: The empty request
            context: The gRPC context
            
        Returns:
            Empty response
        """
        try:
            # Log that we received a health check request
            logger.info("Received health check request")
            return llm_pb2.HealthCheckResponse(status="SERVING")
        except Exception as e:
            logger.error(f"Error in health check: {e}")
            context.abort(grpc.StatusCode.INTERNAL, f"Error in health check: {e}")

    def _get_or_create_session_id(self, context):
        """
        Gets or creates a session ID for sticky routing.
        
        Args:
            context: The gRPC context with metadata
            
        Returns:
            The session ID from metadata or a new one
        """
        if not self.config.sticky_session_enabled:
            return None
            
        try:
            # Try to extract session from metadata (cookie)
            metadata = dict(context.invocation_metadata())
            cookie = metadata.get('cookie', '')
            
            # Parse the cookie string to find our session cookie
            if cookie and self.config.sticky_session_cookie in cookie:
                # Extract the session ID from the cookie string
                cookie_parts = cookie.split(';')
                for part in cookie_parts:
                    if self.config.sticky_session_cookie in part:
                        key_value = part.strip().split('=')
                        if len(key_value) == 2 and key_value[0] == self.config.sticky_session_cookie:
                            return key_value[1]
            
            # If no session cookie found, create a new one
            import uuid
            return str(uuid.uuid4())
                
        except Exception as e:
            logger.warning(f"Error processing session cookie: {e}")
            return None

    def _map_parameters(self, params):
        """
        Map gRPC request parameters to Ollama API parameters.
        
        Args:
            params: Parameters from the gRPC request
            
        Returns:
            Dictionary of parameters for the Ollama API
        """
        # Default values from config if not specified
        temperature = params.temperature if params.temperature != 0 else self.config.default_temperature
        max_tokens = params.max_tokens if params.max_tokens != 0 else self.config.default_max_tokens
        top_p = params.top_p if params.top_p != 0 else self.config.default_top_p
        
        # Map to Ollama's parameter names
        return {
            "temperature": temperature,
            "top_p": top_p,
            "num_predict": max_tokens,
            # Add other parameters as needed
            "presence_penalty": params.presence_penalty,
            "frequency_penalty": params.frequency_penalty,
        }


def serve(config: Config):
    """
    Start the gRPC server with the given configuration.
    
    Args:
        config: Configuration object
    """
    # Configure gRPC server with HTTP/2 options for ALB and sticky sessions
    server_options = [
        # Maximum message size (100MB)
        ('grpc.max_receive_message_length', 100 * 1024 * 1024),
        ('grpc.max_send_message_length', 100 * 1024 * 1024),
        # HTTP/2 settings for ALB
        ('grpc.http2.min_time_between_pings_ms', 10000),
        ('grpc.http2.max_pings_without_data', 0),
        ('grpc.keepalive_time_ms', 30000),
        ('grpc.keepalive_timeout_ms', 10000),
        ('grpc.keepalive_permit_without_calls', 1),
        ('grpc.http2.max_ping_strikes', 0),
    ]
    
    server = grpc.server(
        futures.ThreadPoolExecutor(max_workers=config.worker_threads),
        options=server_options
    )
    
    # Add the LLM service to the server
    service = LLMService(config)
    llm_pb2_grpc.add_LLMServiceServicer_to_server(service, server)
    
    # Add reflection service if enabled
    if config.reflection_enabled:
        logger.info("Enabling gRPC reflection service")
        try:
            # Import reflection service dynamically to avoid dependency when not used
            from grpc_reflection.v1alpha import reflection
            
            # Log the services available in the DESCRIPTOR
            logger.info(f"DESCRIPTOR services: {list(llm_pb2.DESCRIPTOR.services_by_name.keys())}")
            
            # Check if LLMService exists in the DESCRIPTOR
            if 'LLMService' not in llm_pb2.DESCRIPTOR.services_by_name:
                logger.error("ERROR: 'LLMService' not found in DESCRIPTOR services!")
                logger.error("This indicates that the proto files might not be compiled correctly")
                service_name = "llm.LLMService"  # Fallback using string name
            else:
                service = llm_pb2.DESCRIPTOR.services_by_name['LLMService']
                logger.info(f"Service methods: {[m.name for m in service.methods]}")
                service_name = service.full_name
                # Check if HealthCheck exists in the service
                has_health_check = any(m.name == 'HealthCheck' for m in service.methods)
                logger.info(f"Has HealthCheck method: {has_health_check}")
            
            # Get service names for reflection
            service_names = [
                service_name,  # Use the service name found or fallback
                reflection.SERVICE_NAME,
            ]
            
            # Add reflection service to server
            reflection.enable_server_reflection(service_names, server)
            logger.info(f"Reflection successfully enabled for services: {', '.join(service_names)}")
        except ImportError as e:
            logger.error(f"Failed to import grpc_reflection: {e}")
            logger.error("Health checks may fail if reflection is not available - install with: pip install grpcio-reflection")
        except Exception as e:
            logger.error(f"Failed to enable reflection: {e}")
            logger.error(f"Exception type: {type(e).__name__}")
            logger.error(f"Exception details: {str(e)}")
    else:
        logger.warning("gRPC reflection is DISABLED - health checks may fail")
        logger.warning("To enable reflection, set REFLECTION_ENABLED=true")
    
    # Add secure or insecure port based on configuration
    port_str = f'[::]:{config.port}'
    if config.use_tls:
        # Implementation for TLS would go here
        credentials = grpc.ssl_server_credentials([(
            open(config.tls_key_path, 'rb').read(),
            open(config.tls_cert_path, 'rb').read()
        )])
        port = server.add_secure_port(port_str, credentials)
    else:
        port = server.add_insecure_port(port_str)
    
    # Verify port binding was successful
    if port == 0:
        logger.error(f"Failed to bind to port {config.port}")
        sys.exit(1)
    
    # Start the server
    server.start()
    logger.info(f"Server started, listening on port {config.port}")
    logger.info(f"Worker threads: {config.worker_threads}")
    logger.info(f"Using model: {config.model_name}")
    logger.info(f"Reflection enabled: {config.reflection_enabled}")
    
    try:
        # Keep the server running
        while True:
            time.sleep(3600)  # One hour in seconds
    except KeyboardInterrupt:
        logger.info("Shutting down server...")
        server.stop(0)
        logger.info("Server stopped")