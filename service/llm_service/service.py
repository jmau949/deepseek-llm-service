"""
LLM gRPC Service Implementation
------------------------------
Implementation of the LLM gRPC service.
"""

import time
import logging
import grpc
import os
from concurrent import futures
from typing import Dict, Any

from llm_service.config import Config
from llm_service.utils.ollama import OllamaClient, LLMChunk

# Import generated protocol buffer code
# This will be generated by scripts/generate_proto.py
try:
    from proto import llm_pb2, llm_pb2_grpc
except ImportError:
    raise ImportError(
        "Protocol buffer code not found. "
        "Please run 'python scripts/generate_proto.py' first."
    )

logger = logging.getLogger(__name__)


class LLMService(llm_pb2_grpc.LLMServiceServicer):
    """
    gRPC service implementation for LLM inference.
    
    This service provides two endpoints:
    - GenerateStream: Streams the LLM response as it's generated
    - Generate: Returns the complete LLM response
    """
    
    def __init__(self, config: Config):
        """
        Initialize the LLM service with configuration.
        
        Args:
            config: Configuration object containing all settings
        """
        self.config = config
        self.ollama_client = OllamaClient(
            base_url=config.ollama_url,
            model_name=config.model_name,
            timeout=config.request_timeout
        )
        logger.info(
            f"Initialized LLM service with Ollama at {config.ollama_url} "
            f"using model {config.model_name}"
        )

    def GenerateStream(self, request, context):
        """
        Streams the LLM response back to the client.
        
        Args:
            request: The LLMRequest containing the prompt and parameters
            context: The gRPC context
            
        Yields:
            LLMResponse chunks as they are generated
        """
        logger.info(f"Received streaming request with prompt: {request.prompt[:50]}...")
        
        try:
            # Extract or create session ID for sticky sessions
            session_id = self._get_or_create_session_id(context)
            logger.debug(f"Using session ID: {session_id}")
            
            # Set session cookie in metadata for sticky sessions
            if self.config.sticky_session_enabled and session_id:
                context.set_trailing_metadata([
                    ('set-cookie', f'{self.config.sticky_session_cookie}={session_id}; Path=/; Max-Age=900; Secure; HttpOnly')
                ])
            
            # Map gRPC request parameters to Ollama parameters
            ollama_params = self._map_parameters(request.parameters)
            
            # Stream responses from Ollama
            for chunk in self.ollama_client.generate_stream(request.prompt, ollama_params):
                yield llm_pb2.LLMResponse(
                    text=chunk.text,
                    is_complete=chunk.is_complete
                )
                
            logger.info("Streaming response completed")
            
        except Exception as e:
            logger.error(f"Error generating streaming response: {e}")
            context.abort(grpc.StatusCode.INTERNAL, f"Error generating response: {e}")

    def Generate(self, request, context):
        """
        Generates a complete LLM response.
        
        Args:
            request: The LLMRequest containing the prompt and parameters
            context: The gRPC context
            
        Returns:
            LLMCompleteResponse containing the full generated text
        """
        logger.info(f"Received non-streaming request with prompt: {request.prompt[:50]}...")
        
        try:
            # Extract or create session ID for sticky sessions
            session_id = self._get_or_create_session_id(context)
            logger.debug(f"Using session ID: {session_id}")
            
            # Set session cookie in metadata for sticky sessions
            if self.config.sticky_session_enabled and session_id:
                context.set_trailing_metadata([
                    ('set-cookie', f'{self.config.sticky_session_cookie}={session_id}; Path=/; Max-Age=900; Secure; HttpOnly')
                ])
            
            # Map gRPC request parameters to Ollama parameters
            ollama_params = self._map_parameters(request.parameters)
            
            # Get complete response from Ollama
            response = self.ollama_client.generate(request.prompt, ollama_params)
            
            return llm_pb2.LLMCompleteResponse(text=response)
            
        except Exception as e:
            logger.error(f"Error generating complete response: {e}")
            context.abort(grpc.StatusCode.INTERNAL, f"Error generating response: {e}")

    def _get_or_create_session_id(self, context):
        """
        Gets or creates a session ID for sticky routing.
        
        Args:
            context: The gRPC context with metadata
            
        Returns:
            The session ID from metadata or a new one
        """
        if not self.config.sticky_session_enabled:
            return None
            
        try:
            # Try to extract session from metadata (cookie)
            metadata = dict(context.invocation_metadata())
            cookie = metadata.get('cookie', '')
            
            # Parse the cookie string to find our session cookie
            if cookie and self.config.sticky_session_cookie in cookie:
                # Extract the session ID from the cookie string
                cookie_parts = cookie.split(';')
                for part in cookie_parts:
                    if self.config.sticky_session_cookie in part:
                        key_value = part.strip().split('=')
                        if len(key_value) == 2 and key_value[0] == self.config.sticky_session_cookie:
                            return key_value[1]
            
            # If no session cookie found, create a new one
            import uuid
            return str(uuid.uuid4())
                
        except Exception as e:
            logger.warning(f"Error processing session cookie: {e}")
            return None

    def _map_parameters(self, params):
        """
        Map gRPC request parameters to Ollama API parameters.
        
        Args:
            params: Parameters from the gRPC request
            
        Returns:
            Dictionary of parameters for the Ollama API
        """
        # Default values from config if not specified
        temperature = params.temperature if params.temperature != 0 else self.config.default_temperature
        max_tokens = params.max_tokens if params.max_tokens != 0 else self.config.default_max_tokens
        top_p = params.top_p if params.top_p != 0 else self.config.default_top_p
        
        # Map to Ollama's parameter names
        return {
            "temperature": temperature,
            "top_p": top_p,
            "num_predict": max_tokens,
            # Add other parameters as needed
            "presence_penalty": params.presence_penalty,
            "frequency_penalty": params.frequency_penalty,
        }


def serve(config: Config):
    """
    Start the gRPC server with the given configuration.
    
    Args:
        config: Configuration object
    """
    # Configure gRPC server with HTTP/2 options for ALB and sticky sessions
    server_options = [
        # Maximum message size (100MB)
        ('grpc.max_receive_message_length', 100 * 1024 * 1024),
        ('grpc.max_send_message_length', 100 * 1024 * 1024),
        # HTTP/2 settings for ALB
        ('grpc.http2.min_time_between_pings_ms', 10000),
        ('grpc.http2.max_pings_without_data', 0),
        ('grpc.keepalive_time_ms', 30000),
        ('grpc.keepalive_timeout_ms', 10000),
        ('grpc.keepalive_permit_without_calls', 1),
        ('grpc.http2.max_ping_strikes', 0),
    ]
    
    server = grpc.server(
        futures.ThreadPoolExecutor(max_workers=config.worker_threads),
        options=server_options
    )
    
    # Add the LLM service to the server
    llm_pb2_grpc.add_LLMServiceServicer_to_server(
        LLMService(config), server
    )
    
    # Add secure or insecure port based on configuration
    if config.use_tls:
        # Implementation for TLS would go here
        credentials = grpc.ssl_server_credentials([(
            open(config.tls_key_path, 'rb').read(),
            open(config.tls_cert_path, 'rb').read()
        )])
        server.add_secure_port(f'[::]:{config.port}', credentials)
    else:
        server.add_insecure_port(f'[::]:{config.port}')
    
    # Start the server
    server.start()
    logger.info(f"Server started, listening on port {config.port}")
    
    try:
        # Keep the server running
        while True:
            time.sleep(3600)  # One hour in seconds
    except KeyboardInterrupt:
        logger.info("Shutting down server...")
        server.stop(0)
        logger.info("Server stopped")